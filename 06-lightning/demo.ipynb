{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from model import MNISTClassifier, LightningMNISTClassifier\n",
    "from data import mnist_dataloaders, MNISTDataModule\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "MNISTClassifier(\n  (layer_1): Linear(in_features=784, out_features=128, bias=True)\n  (layer_2): Linear(in_features=128, out_features=256, bias=True)\n  (layer_3): Linear(in_features=256, out_features=10, bias=True)\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model = MNISTClassifier()\n",
    "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=1e-3)\n",
    "pytorch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    return F.nll_loss(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LightningMNISTClassifier(\n  (backbone): MNISTClassifier(\n    (layer_1): Linear(in_features=784, out_features=128, bias=True)\n    (layer_2): Linear(in_features=128, out_features=256, bias=True)\n    (layer_3): Linear(in_features=256, out_features=10, bias=True)\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_model = LightningMNISTClassifier()\n",
    "lightning_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_val, mnist_test = mnist_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = MNISTDataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, epochs, train_data, val_data):\n",
    "    device = next(pytorch_model.parameters()).device\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        # TRAINING LOOP\n",
    "        for train_batch in train_data:\n",
    "            x, y = train_batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = cross_entropy_loss(logits, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # VALIDATION LOOP\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            for val_batch in val_data:\n",
    "                x, y = val_batch\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits = model(x)\n",
    "                val_loss.append(cross_entropy_loss(logits, y).item())\n",
    "            val_loss = torch.mean(torch.tensor(val_loss))\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            'Epoch {} in {:.03f} secs: train loss: {:.05f}, val loss: {:.05f}'.format(\n",
    "                epoch, end_time - start_time, loss.item(), val_loss.item()\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data):\n",
    "    device = next(pytorch_model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        test_loss = []\n",
    "        test_acc = []\n",
    "        for test_batch in test_data:\n",
    "            x, y = test_batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            labels_hat = torch.argmax(logits, dim=1)\n",
    "            test_loss.append(cross_entropy_loss(logits, y).item())\n",
    "            test_acc.append(torch.sum(y == labels_hat).item() / (len(y) * 1.0))\n",
    "        test_loss = torch.mean(torch.tensor(test_loss))\n",
    "        test_acc = torch.mean(torch.tensor(test_acc))\n",
    "\n",
    "    print('Acc: {}, Loss: {}'.format(test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 in 27.706 secs: train loss: 0.18244, val loss: 0.14465\n",
      "Epoch 2 in 27.193 secs: train loss: 0.17212, val loss: 0.11673\n",
      "Epoch 3 in 25.817 secs: train loss: 0.07339, val loss: 0.11028\n",
      "Epoch 4 in 27.108 secs: train loss: 0.05643, val loss: 0.12163\n",
      "Epoch 5 in 27.289 secs: train loss: 0.07815, val loss: 0.12740\n",
      "Acc: 0.9722332954406738, Loss: 0.1009707972407341\n"
     ]
    }
   ],
   "source": [
    "training_loop(pytorch_model, optimizer, 5, mnist_train, mnist_val)\n",
    "test_model(pytorch_model, mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 in 26.141 secs: train loss: 0.15747, val loss: 0.14148\n",
      "Epoch 2 in 28.221 secs: train loss: 0.14932, val loss: 0.11948\n",
      "Epoch 3 in 24.991 secs: train loss: 0.09565, val loss: 0.11738\n",
      "Epoch 4 in 24.814 secs: train loss: 0.03464, val loss: 0.12288\n",
      "Epoch 5 in 38.835 secs: train loss: 0.17945, val loss: 0.13479\n",
      "Acc: 0.9663614630699158, Loss: 0.12964561581611633\n"
     ]
    }
   ],
   "source": [
    "pytorch_model = MNISTClassifier().to('cuda')\n",
    "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(pytorch_model, optimizer, 5, mnist_train, mnist_val)\n",
    "test_model(pytorch_model, mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | backbone | MNISTClassifier | 136 K \n",
      "---------------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 939/939 [00:38<00:00, 24.29it/s, loss=0.0363, v_num=5]\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=5, default_root_dir='runs')\n",
    "\n",
    "trainer.fit(lightning_model, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 157/157 [00:06<00:00, 22.74it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9740999937057495, 'test_loss': 0.10101911425590515}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer.test(lightning_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | backbone | MNISTClassifier | 136 K \n",
      "---------------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 939/939 [00:41<00:00, 22.46it/s, loss=0.0277, v_num=6]\n",
      "Testing: 100%|██████████| 157/157 [00:06<00:00, 24.58it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9690999984741211, 'test_loss': 0.1238018050789833}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lightning_model = LightningMNISTClassifier(backbone=MNISTClassifier())\n",
    "trainer = pl.Trainer(max_epochs=5, default_root_dir='runs', gpus=1)\n",
    "\n",
    "trainer.fit(lightning_model, mnist_data)\n",
    "trainer.test(lightning_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPrintingCallback(pl.Callback):\n",
    "    def on_init_start(self, trainer):\n",
    "        print('Starting to init trainer!')\n",
    "\n",
    "    def on_init_end(self, trainer):\n",
    "        print('trainer is init now')\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print('do something when training ends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | backbone | MNISTClassifier | 136 K \n",
      "---------------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to init trainer!\n",
      "trainer is init now\n",
      "Epoch 1: 100%|██████████| 939/939 [00:40<00:00, 23.07it/s, loss=0.0959, v_num=7]do something when training ends\n",
      "Epoch 1: 100%|██████████| 939/939 [00:40<00:00, 23.05it/s, loss=0.0959, v_num=7]\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_model = LightningMNISTClassifier()\n",
    "trainer = pl.Trainer(max_epochs=1, default_root_dir='runs', gpus=1, callbacks=[MyPrintingCallback()])\n",
    "\n",
    "trainer.fit(lightning_model, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | backbone | MNISTClassifier | 136 K \n",
      "---------------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 939/939 [00:36<00:00, 25.75it/s, loss=0.0988, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  93.694         \t|  100 %          \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  46.723         \t|2              \t|  93.445         \t|  99.734         \t|\n",
      "get_train_batch                    \t|  0.037963       \t|1720           \t|  65.297         \t|  69.691         \t|\n",
      "run_training_batch                 \t|  0.0077699      \t|1720           \t|  13.364         \t|  14.264         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.0063745      \t|1720           \t|  10.964         \t|  11.702         \t|\n",
      "training_step_and_backward         \t|  0.0043909      \t|1720           \t|  7.5524         \t|  8.0606         \t|\n",
      "on_train_batch_end                 \t|  0.0030049      \t|1720           \t|  5.1685         \t|  5.5163         \t|\n",
      "model_forward                      \t|  0.0023072      \t|1720           \t|  3.9684         \t|  4.2355         \t|\n",
      "training_step                      \t|  0.0016411      \t|1720           \t|  2.8227         \t|  3.0127         \t|\n",
      "model_backward                     \t|  0.0015137      \t|1720           \t|  2.6036         \t|  2.7788         \t|\n",
      "cache_result                       \t|  4.002e-05      \t|9109           \t|  0.36454        \t|  0.38908        \t|\n",
      "on_validation_batch_end            \t|  0.001794       \t|160            \t|  0.28704        \t|  0.30636        \t|\n",
      "evaluation_step_and_end            \t|  0.0017284      \t|160            \t|  0.27654        \t|  0.29515        \t|\n",
      "validation_step                    \t|  0.0013643      \t|160            \t|  0.21828        \t|  0.23298        \t|\n",
      "on_batch_start                     \t|  5.3882e-05     \t|1720           \t|  0.092676       \t|  0.098914       \t|\n",
      "on_batch_end                       \t|  4.2723e-05     \t|1720           \t|  0.073484       \t|  0.07843        \t|\n",
      "on_before_zero_grad                \t|  4.2309e-05     \t|1720           \t|  0.072772       \t|  0.07767        \t|\n",
      "on_after_backward                  \t|  3.985e-05      \t|1720           \t|  0.068542       \t|  0.073155       \t|\n",
      "training_step_end                  \t|  3.0461e-05     \t|1720           \t|  0.052393       \t|  0.055919       \t|\n",
      "on_train_batch_start               \t|  2.554e-05      \t|1720           \t|  0.04393        \t|  0.046886       \t|\n",
      "on_validation_end                  \t|  0.014571       \t|3              \t|  0.043713       \t|  0.046655       \t|\n",
      "on_train_end                       \t|  0.013217       \t|1              \t|  0.013217       \t|  0.014106       \t|\n",
      "on_validation_start                \t|  0.0041955      \t|3              \t|  0.012586       \t|  0.013433       \t|\n",
      "on_epoch_start                     \t|  0.004036       \t|2              \t|  0.008072       \t|  0.0086152      \t|\n",
      "on_validation_batch_start          \t|  4.0854e-05     \t|160            \t|  0.0065367      \t|  0.0069766      \t|\n",
      "validation_step_end                \t|  2.3112e-05     \t|160            \t|  0.003698       \t|  0.0039469      \t|\n",
      "on_train_start                     \t|  0.0026577      \t|1              \t|  0.0026577      \t|  0.0028365      \t|\n",
      "on_fit_start                       \t|  0.00011416     \t|1              \t|  0.00011416     \t|  0.00012184     \t|\n",
      "on_validation_epoch_end            \t|  2.2615e-05     \t|3              \t|  6.7846e-05     \t|  7.2412e-05     \t|\n",
      "on_epoch_end                       \t|  1.2391e-05     \t|5              \t|  6.1955e-05     \t|  6.6125e-05     \t|\n",
      "on_train_epoch_start               \t|  2.8605e-05     \t|2              \t|  5.7209e-05     \t|  6.1059e-05     \t|\n",
      "on_validation_epoch_start          \t|  1.4851e-05     \t|3              \t|  4.4552e-05     \t|  4.755e-05      \t|\n",
      "on_train_epoch_end                 \t|  1.9692e-05     \t|2              \t|  3.9384e-05     \t|  4.2035e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  1.0821e-05     \t|1              \t|  1.0821e-05     \t|  1.1549e-05     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_model = LightningMNISTClassifier()\n",
    "trainer = pl.Trainer(max_epochs=1, default_root_dir='runs', gpus=1, profiler=True)\n",
    "\n",
    "trainer.fit(lightning_model, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f616a2eaf358a79ac637e0cf8897ada8031c3d6a61073341b7dba63781707543"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}